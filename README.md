# Notes on Deep Learning

## from the Author

These are the notes that I left working through Nielson's [neural Net and Deep Learning book](https://neuralnetworksanddeeplearning.com). You can find a table of contents of this repo below.

## Table of Contents
### Chapter 1: Intro to Deep Learning
- [001 - sigmoid function](Ch1%20intro%20to%20deep%20learning/001%20-%20sigmoid%20function.ipynb)
- [002 - training a single perceptron](Ch1%20intro%20to%20deep%20learning/002%20-%20training%20a%20single%20perceptron.ipynb)
- [003 - use perceptrons to target arbitrary function](Ch1%20intro%20to%20deep%20learning/003%20-%20use%20perceptrons%20to%20target%20arbitrary%20function.ipynb)
- [004 - optimize batch training](Ch1%20intro%20to%20deep%20learning/004%20-%20optimize%20batch%20training.ipynb)

### Chapter 2: Intro to Tensorflow
- [005 - Tensorflow Intro](Ch2%20Intro%20to%20Tensorflow/005%20-%20Tensorflow%20Intro.ipynb)
- [006 - Tensorflow Softmax Regression](Ch2%20Intro%20to%20Tensorflow/006%20-%20Tensorflow%20Softmax%20Regression.ipynb)

## Fun Highlights

some of the figures can be found scattered in the folder (I believe in a flat folder 
structure). Like this one below, it shows how a simple network can be trained 
to emulate a given target function.

[![network trained to emulate function](trained%20neural%20net%20emulate%20a%20step%20function.png)](004%20-%20optimize%20batch%20training.ipynb)

## Todos:

- [ ] add convNet MNIST example with Tensorflow

### Done:

- [x] work on optimize batch training.
